# Perceiver ëª¨ë¸ í•™ìŠµ ì½”ë“œ

PyTorchë¡œ êµ¬í˜„í•œ Perceiver ëª¨ë¸ì˜ ì™„ì „í•œ í•™ìŠµ ë° í‰ê°€ ì½”ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

PerceiverëŠ” DeepMindì—ì„œ ì œì•ˆí•œ ë²”ìš© ì•„í‚¤í…ì²˜ë¡œ, ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **Cross-attention**: ì…ë ¥ì—ì„œ ê³ ì • í¬ê¸° latentë¡œ ì •ë³´ ì••ì¶•
- **Self-attention**: Latent ê³µê°„ì—ì„œ ì •ë³´ ì²˜ë¦¬
- **í™•ì¥ì„±**: ì…ë ¥ í¬ê¸°ì— ê´€ê³„ì—†ì´ ê³„ì‚°ëŸ‰ ì¼ì •

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from perceiver_model import create_perceiver_model

# ëª¨ë¸ ìƒì„±
model = create_perceiver_model(
    input_dim=32,      # ì…ë ¥ íŠ¹ì„± ì°¨ì›
    num_classes=10,    # ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜
    model_size='base'  # 'small', 'base', 'large'
)

# ì¶”ë¡ 
import torch
x = torch.randn(4, 100, 32)  # (ë°°ì¹˜, ì‹œí€€ìŠ¤ê¸¸ì´, íŠ¹ì„±ì°¨ì›)
output = model(x)  # (ë°°ì¹˜, í´ë˜ìŠ¤ìˆ˜)
```

### 2. ì˜ˆì œ ì‹¤í–‰

```bash
# ë‹¤ì–‘í•œ ì˜ˆì œ ì‹¤í–‰
python example_usage.py

# í•©ì„± ë°ì´í„°ë¡œ ê°„ë‹¨í•œ í•™ìŠµ
python train.py --dataset synthetic --epochs 10 --batch_size 16
```

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
Perceiver/
â”œâ”€â”€ perceiver_model.py    # ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„
â”œâ”€â”€ data_utils.py         # ë°ì´í„° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference.py         # ì¶”ë¡  ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ example_usage.py     # ì‚¬ìš© ì˜ˆì œë“¤
â””â”€â”€ README.md            # ì´ íŒŒì¼
```

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### ì£¼ìš” êµ¬ì„± ìš”ì†Œ

1. **MultiHeadAttention**: í‘œì¤€ ë©€í‹°í—¤ë“œ ì–´í…ì…˜
2. **CrossAttentionBlock**: ì…ë ¥â†’latent ì •ë³´ ì „ë‹¬
3. **SelfAttentionBlock**: Latent ê³µê°„ ë‚´ ì •ë³´ ì²˜ë¦¬
4. **PerceiverEncoder**: Cross + Self attention ë°˜ë³µ
5. **PerceiverModel**: ì™„ì „í•œ ë¶„ë¥˜ ëª¨ë¸

### ëª¨ë¸ í¬ê¸°ë³„ ì„¤ì •

| í¬ê¸°   | d_model | heads | latents | layers (cross/self) |
|--------|---------|-------|---------|-------------------|
| small  | 256     | 4     | 128     | 1 / 3             |
| base   | 512     | 8     | 256     | 1 / 6             |  
| large  | 768     | 12    | 512     | 2 / 8             |

## ğŸ“Š ë°ì´í„° ì²˜ë¦¬

### ì§€ì›í•˜ëŠ” ë°ì´í„° íƒ€ì…

1. **í•©ì„± ì‹œí€€ìŠ¤ ë°ì´í„°**: íŒ¨í„´ë³„ ì‹œê³„ì—´ ë°ì´í„°
2. **ì´ë¯¸ì§€â†’íŒ¨ì¹˜**: CIFAR-10ì„ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
3. **ì»¤ìŠ¤í…€ ì‹œí€€ìŠ¤**: ì‚¬ìš©ì ì •ì˜ ì‹œí€€ìŠ¤ ë°ì´í„°

### ë°ì´í„° ì˜ˆì œ

```python
from data_utils import create_synthetic_sequence_dataloader

# í•©ì„± ë°ì´í„° ìƒì„±
dataloader = create_synthetic_sequence_dataloader(
    batch_size=32,
    num_samples=1000,
    seq_len_range=(50, 200),
    feature_dim=64,
    num_classes=5
)

# CIFAR-10 íŒ¨ì¹˜ ë°ì´í„°
from data_utils import create_cifar10_sequence_dataset
cifar_loader = create_cifar10_sequence_dataset(
    batch_size=32,
    patch_size=8
)
```

## ğŸ¯ í•™ìŠµí•˜ê¸°

### ê¸°ë³¸ í•™ìŠµ

```bash
# í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ
python train.py --dataset synthetic --model_size base --epochs 50

# CIFAR-10ìœ¼ë¡œ í•™ìŠµ  
python train.py --dataset cifar10 --model_size small --epochs 100 --lr 1e-4
```

### ê³ ê¸‰ ì˜µì…˜

```bash
python train.py \
    --dataset cifar10 \
    --model_size large \
    --batch_size 64 \
    --epochs 200 \
    --lr 2e-4 \
    --config config.json
```

### ì„¤ì • íŒŒì¼ ì˜ˆì œ (config.json)

```json
{
    "batch_size": 32,
    "learning_rate": 1e-4,
    "num_epochs": 100,
    "weight_decay": 1e-4,
    "model_size": "base",
    "dataset_type": "synthetic",
    "seq_len_range": [50, 200],
    "num_samples": 5000
}
```

## ğŸ“ˆ ëª¨ë¸ í‰ê°€

### ê¸°ë³¸ í‰ê°€

```bash
# í•™ìŠµëœ ëª¨ë¸ í‰ê°€
python inference.py --model_path checkpoints/best_model.pth

# ìƒì„¸ ë¶„ì„ í¬í•¨
python inference.py \
    --model_path checkpoints/best_model.pth \
    --analyze \
    --output_dir results/
```

### í‰ê°€ ê²°ê³¼

í‰ê°€ ì‹œ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:
- `confusion_matrix.png`: í˜¼ë™ í–‰ë ¬
- `confidence_histogram.png`: ì‹ ë¢°ë„ ë¶„í¬
- `confidence_analysis.png`: ì‹ ë¢°ë„ë³„ ì •í™•ë„ ë¶„ì„
- `results.json`: ìˆ˜ì¹˜ ê²°ê³¼

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€

```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
    
    def __getitem__(self, idx):
        return {
            'input': self.data[idx],
            'label': self.labels[idx],
            'mask': torch.ones(len(self.data[idx]))
        }
```

### ëª¨ë¸ êµ¬ì¡° ìˆ˜ì •

```python
# ì»¤ìŠ¤í…€ Perceiver ì„¤ì •
model = PerceiverModel(
    input_dim=128,
    d_model=512,
    num_heads=8,
    num_cross_attention_layers=2,  # Cross-attention ë ˆì´ì–´ ìˆ˜ ì¦ê°€
    num_self_attention_layers=8,   # Self-attention ë ˆì´ì–´ ìˆ˜ ì¦ê°€
    num_latents=512,               # Latent ìˆ˜ ì¦ê°€
    num_classes=1000
)
```

## ğŸ“š ì£¼ìš” ê¸°ëŠ¥

### 1. ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
- ìë™ íŒ¨ë”© ë° ë§ˆìŠ¤í‚¹
- íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬

### 2. ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì§€ì›  
- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì‹œê³„ì—´ ë“±
- í†µí•©ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 3. í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
- ëª¨ë“ˆí™”ëœ ì„¤ê³„
- ì‰¬ìš´ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 4. ì™„ë²½í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸
- TensorBoard ë¡œê¹…
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
- Learning rate scheduling
- Gradient clipping

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

1. **ë°°ì¹˜ í¬ê¸°**: GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
2. **Latent ìˆ˜**: ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„
3. **ì‹œí€€ìŠ¤ ê¸¸ì´**: ë„ˆë¬´ ê¸´ ì‹œí€€ìŠ¤ëŠ” ì˜ë¼ë‚´ê¸°
4. **Mixed Precision**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ì—ì„œ ë©”ëª¨ë¦¬ ì ˆì•½

```python
# Mixed precision ì˜ˆì œ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(inputs)
    loss = criterion(output, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   - ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   - ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸° ('small' ì‚¬ìš©)
   - Gradient checkpointing ì‚¬ìš©

2. **í•™ìŠµì´ ëŠë¦° ê²½ìš°**
   - ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
   - Latent ìˆ˜ ì¤„ì´ê¸°
   - ë” ì‘ì€ ëª¨ë¸ í¬ê¸° ì‚¬ìš©

3. **ì •í™•ë„ê°€ ë‚®ì€ ê²½ìš°**
   - Learning rate ì¡°ì •
   - ë” ë§ì€ ì—í¬í¬ í•™ìŠµ
   - ëª¨ë¸ í¬ê¸° ì¦ê°€

## ğŸ“– ì°¸ê³  ìë£Œ

- [Perceiver ë…¼ë¬¸](https://arxiv.org/abs/2103.03206)
- [Perceiver IO ë…¼ë¬¸](https://arxiv.org/abs/2107.14795)
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê¸°ëŠ¥ ê°œì„  ì œì•ˆì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤!

---

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

ì´ ì½”ë“œëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.