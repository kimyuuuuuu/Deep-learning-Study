import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import math

from module.Scaled_dot_product_Attention import ScaleDotProductAttention

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

class SimpleTokenizer:
    def __init__(self):
        # ê°„ë‹¨í•œ vocabulary
        self.vocab = {
            '<PAD>': 0, 'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5,
            'dog': 6, 'ran': 7, 'to': 8, 'park': 9, 'and': 10, 'played': 11
        }
        self.id_to_token = {v: k for k, v in self.vocab.items()}
    
    def tokenize(self, sentence):
        tokens = sentence.lower().split()
        return [self.vocab.get(token, 0) for token in tokens]
    
    def decode(self, ids):
        return [self.id_to_token.get(id, '<UNK>') for id in ids]

def create_simple_embeddings(vocab_size=20, d_model=64):
    """ê°„ë‹¨í•œ word embedding ìƒì„± (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ê²ƒ ì‚¬ìš©)"""
    # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ seed ì„¤ì •
    torch.manual_seed(42)
    embedding = nn.Embedding(vocab_size, d_model)
    return embedding

def visualize_attention_simple():
    """ê°€ì¥ ê°„ë‹¨í•œ attention ì‹œê°í™”"""
    
    # 1. ë°ì´í„° ì¤€ë¹„
    tokenizer = SimpleTokenizer()
    sentence = "the cat sat on the mat"
    tokens = tokenizer.tokenize(sentence)
    token_names = tokenizer.decode(tokens)
    
    print(f"ë¬¸ì¥: {sentence}")
    print(f"í† í°ë“¤: {token_names}")
    print(f"í† í° IDs: {tokens}")
    
    # 2. Embedding ìƒì„±
    d_model = 64
    embedding = create_simple_embeddings(d_model=d_model)
    
    # í† í°ì„ embeddingìœ¼ë¡œ ë³€í™˜
    token_tensor = torch.tensor(tokens).unsqueeze(0)  # [1, seq_len]
    embeddings = embedding(token_tensor)  # [1, seq_len, d_model]
    
    # 3. Multi-head attentionì„ ìœ„í•œ ì°¨ì› ë³€í™˜
    num_heads = 4
    d_k = d_model // num_heads
    seq_len = len(tokens)
    
    # Q, K, V ìƒì„± (ê°„ë‹¨íˆ ê°™ì€ embedding ì‚¬ìš©)
    # ì‹¤ì œë¡œëŠ” ì„œë¡œ ë‹¤ë¥¸ linear projection ì‚¬ìš©
    q = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)  # [1, num_heads, seq_len, d_k]
    k = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)
    v = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)
    
    # 4. Attention ê³„ì‚°
    attention_layer = ScaleDotProductAttention()
    output, attention_weights = attention_layer(q, k, v)
    
    # 5. ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle(f'Attention Weights for: "{sentence}"', fontsize=16)
    
    # ê° headë³„ë¡œ ì‹œê°í™”
    for head in range(num_heads):
        row = head // 2
        col = head % 2
        
        # attention weights ì¶”ì¶œ [seq_len, seq_len]
        attn_matrix = attention_weights[0, head].detach().numpy()
        
        # Heatmap ê·¸ë¦¬ê¸°
        sns.heatmap(attn_matrix, 
                   xticklabels=token_names,
                   yticklabels=token_names,
                   annot=True,
                   fmt='.3f',
                   cmap='Blues',
                   ax=axes[row, col])
        
        axes[row, col].set_title(f'Head {head + 1}')
        axes[row, col].set_xlabel('Key (attending to)')
        axes[row, col].set_ylabel('Query (attending from)')
    
    plt.tight_layout()
    plt.show()
    
    # 6. ì „ì²´ í‰ê·  attentionë„ ë³´ê¸°
    plt.figure(figsize=(8, 6))
    avg_attention = attention_weights.mean(dim=1)[0].detach().numpy()  # ëª¨ë“  head í‰ê· 
    
    sns.heatmap(avg_attention,
               xticklabels=token_names,
               yticklabels=token_names,
               annot=True,
               fmt='.3f',
               cmap='Reds')
    
    plt.title('Average Attention Across All Heads')
    plt.xlabel('Key (attending to)')
    plt.ylabel('Query (attending from)')
    plt.show()
    
    return attention_weights, token_names

def analyze_attention_patterns(attention_weights, tokens):
    """Attention íŒ¨í„´ ë¶„ì„"""
    
    print("\n=== Attention íŒ¨í„´ ë¶„ì„ ===")
    
    # ê° ë‹¨ì–´ê°€ ê°€ì¥ ë§ì´ ì£¼ëª©í•˜ëŠ” ë‹¨ì–´ ì°¾ê¸°
    avg_attention = attention_weights.mean(dim=1)[0].detach().numpy()
    
    for i, token in enumerate(tokens):
        # ìê¸° ìì‹  ì œì™¸í•˜ê³  ê°€ì¥ ë†’ì€ attentionì„ ë°›ëŠ” ë‹¨ì–´
        attention_scores = avg_attention[i].copy()
        attention_scores[i] = 0  # ìê¸° ìì‹  ì œì™¸
        
        max_idx = np.argmax(attention_scores)
        max_score = attention_scores[max_idx]
        
        print(f"'{token}' -> '{tokens[max_idx]}' (attention: {max_score:.3f})")
    
    print(f"\nê° ìœ„ì¹˜ì˜ attention í•©ê³„ (ëª¨ë‘ 1.0ì´ì–´ì•¼ í•¨):")
    for i, token in enumerate(tokens):
        attention_sum = avg_attention[i].sum()
        print(f"'{token}': {attention_sum:.3f}")

def compare_different_sentences():
    """ì—¬ëŸ¬ ë¬¸ì¥ì˜ attention íŒ¨í„´ ë¹„êµ"""
    
    sentences = [
        "the cat sat on the mat",
        "the dog ran to the park", 
        "the cat and dog played"
    ]
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    tokenizer = SimpleTokenizer()
    embedding = create_simple_embeddings()
    attention_layer = ScaleDotProductAttention()
    
    for idx, sentence in enumerate(sentences):
        tokens = tokenizer.tokenize(sentence)
        token_names = tokenizer.decode(tokens)
        
        # Embedding ë° attention ê³„ì‚°
        token_tensor = torch.tensor(tokens).unsqueeze(0)
        embeddings = embedding(token_tensor)
        
        seq_len = len(tokens)
        d_model = 64
        num_heads = 4
        d_k = d_model // num_heads
        
        q = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)
        k = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)  
        v = embeddings.view(1, seq_len, num_heads, d_k).transpose(1, 2)
        
        _, attention_weights = attention_layer(q, k, v)
        avg_attention = attention_weights.mean(dim=1)[0].detach().numpy()
        
        # ì‹œê°í™”
        sns.heatmap(avg_attention,
                   xticklabels=token_names,
                   yticklabels=token_names,
                   annot=True,
                   fmt='.2f',
                   cmap='Blues',
                   ax=axes[idx])
        
        axes[idx].set_title(f'"{sentence}"')
        axes[idx].set_xlabel('Key')
        axes[idx].set_ylabel('Query')
    
    plt.tight_layout()
    plt.show()

# ì‹¤í–‰ í•¨ìˆ˜ë“¤
if __name__ == "__main__":
    print("ğŸ¯ Attention Score ì‹œê°í™” ì‹¤ìŠµ")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ ì‹œê°í™”
    print("1. ê¸°ë³¸ attention ì‹œê°í™”")
    attention_weights, tokens = visualize_attention_simple()
    
    # 2. íŒ¨í„´ ë¶„ì„  
    analyze_attention_patterns(attention_weights, tokens)
    
    # 3. ì—¬ëŸ¬ ë¬¸ì¥ ë¹„êµ
    print("\n2. ì—¬ëŸ¬ ë¬¸ì¥ attention íŒ¨í„´ ë¹„êµ")
    compare_different_sentences()