import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import math
from sklearn.metrics import accuracy_score
import time

from module.MultiHead_Attention import MultiHeadAttention

# 1. ê¸°ë³¸ ê²€ì¦: ì°¨ì› í™•ì¸
def test_dimensions():
    """MultiHeadAttentionì˜ ì…ì¶œë ¥ ì°¨ì›ì´ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦"""
    print("=== 1. ì°¨ì› ê²€ì¦ ===")
    
    batch_size, seq_len, d_model = 2, 10, 512
    num_heads = 8
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    x = torch.randn(batch_size, seq_len, d_model)
    
    # MultiHeadAttention ìƒì„±
    mha = MultiHeadAttention(d_model, num_heads)
    
    # Forward pass
    output, attn_weights = mha(x, x, x)
    
    print(f"ì…ë ¥ ì°¨ì›: {x.shape}")
    print(f"ì¶œë ¥ ì°¨ì›: {output.shape}")
    print(f"Attention weights ì°¨ì›: {attn_weights.shape}")
    
    # ê²€ì¦
    assert output.shape == x.shape, f"ì¶œë ¥ ì°¨ì› ë¶ˆì¼ì¹˜: {output.shape} != {x.shape}"
    assert attn_weights.shape == (batch_size, num_heads, seq_len, seq_len), \
           f"Attention weights ì°¨ì› ë¶ˆì¼ì¹˜: {attn_weights.shape}"
    
    print("âœ… ì°¨ì› ê²€ì¦ í†µê³¼!")
    return mha, x, attn_weights

# 2. Headë³„ íŒ¨í„´ ì‹œê°í™”
def visualize_head_patterns(mha, x, attn_weights, sentence_tokens=None):
    """ê° headê°€ í•™ìŠµí•˜ëŠ” attention pattern ì‹œê°í™”"""
    print("\n=== 2. Headë³„ íŒ¨í„´ ì‹œê°í™” ===")
    
    if sentence_tokens is None:
        sentence_tokens = [f"token_{i}" for i in range(x.size(1))]
    
    num_heads = attn_weights.size(1)
    
    # Headë³„ ì‹œê°í™”
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    fig.suptitle('ê° Headë³„ Attention Patterns', fontsize=16)
    
    for head in range(num_heads):
        row = head // 4
        col = head % 4
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ attention matrix
        attn_matrix = attn_weights[0, head].detach().numpy()
        
        sns.heatmap(attn_matrix,
                   xticklabels=sentence_tokens,
                   yticklabels=sentence_tokens,
                   annot=True,
                   fmt='.2f',
                   cmap='Blues',
                   ax=axes[row, col])
        
        axes[row, col].set_title(f'Head {head + 1}')
        axes[row, col].set_xlabel('Key (attending to)')
        axes[row, col].set_ylabel('Query (attending from)')
    
    plt.tight_layout()
    plt.show()
    
    # Headë³„ íŠ¹ì„± ë¶„ì„
    analyze_head_characteristics(attn_weights, sentence_tokens)

def analyze_head_characteristics(attn_weights, tokens):
    """ê° headì˜ íŠ¹ì„± ë¶„ì„"""
    print("\n=== Headë³„ íŠ¹ì„± ë¶„ì„ ===")
    
    num_heads = attn_weights.size(1)
    seq_len = attn_weights.size(2)
    
    for head in range(num_heads):
        attn_matrix = attn_weights[0, head].detach().numpy()
        
        # Self-attention ë¹„ìœ¨
        self_attn = np.diag(attn_matrix).mean()
        
        # Attention ë¶„ì‚°ë„ (entropy)
        entropy = -np.sum(attn_matrix * np.log(attn_matrix + 1e-8), axis=-1).mean()
        
        # ìµœëŒ€ attention í‰ê· 
        max_attn = attn_matrix.max(axis=-1).mean()
        
        print(f"Head {head + 1}:")
        print(f"  Self-attention í‰ê· : {self_attn:.3f}")
        print(f"  Attention ì—”íŠ¸ë¡œí”¼: {entropy:.3f} (ë‚®ì„ìˆ˜ë¡ ì§‘ì¤‘ì )")
        print(f"  ìµœëŒ€ attention í‰ê· : {max_attn:.3f}")
        print()

# 3. Head ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ
def compare_head_numbers():
    """Head ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ê´€ì°°"""
    print("=== 3. Head ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ===")
    
    d_model = 512
    seq_len = 20
    batch_size = 32
    num_epochs = 10
    
    # í…ŒìŠ¤íŠ¸í•  head ìˆ˜ë“¤
    head_numbers = [1, 2, 4, 8, 16]
    results = []
    
    for num_heads in head_numbers:
        print(f"\nğŸ”¹ Testing {num_heads} heads...")
        
        # ê°„ë‹¨í•œ ë¶„ë¥˜ íƒœìŠ¤í¬ ì„¤ì •
        model = SimpleClassifier(d_model, num_heads, num_classes=2)
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)
        train_data = create_dummy_classification_data(batch_size * 10, seq_len, d_model)
        
        # í›ˆë ¨ ë° í‰ê°€
        accuracy, training_time = train_and_evaluate(model, train_data, num_epochs)
        
        results.append({
            'num_heads': num_heads,
            'accuracy': accuracy,
            'training_time': training_time,
            'params': count_parameters(model)
        })
        
        print(f"  ì •í™•ë„: {accuracy:.3f}")
        print(f"  í›ˆë ¨ ì‹œê°„: {training_time:.2f}s")
        print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {count_parameters(model):,}")
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_head_comparison(results)
    return results

def create_dummy_classification_data(num_samples, seq_len, d_model):
    """ë”ë¯¸ ë¶„ë¥˜ ë°ì´í„° ìƒì„±"""
    X = torch.randn(num_samples, seq_len, d_model)
    # ê°„ë‹¨í•œ íŒ¨í„´: ì²« ë²ˆì§¸ í† í°ì˜ í•©ì´ ì–‘ìˆ˜ë©´ í´ë˜ìŠ¤ 1, ìŒìˆ˜ë©´ í´ë˜ìŠ¤ 0
    y = (X[:, 0, :].sum(dim=1) > 0).long()
    return X, y

class SimpleClassifier(nn.Module):
    """MultiHeadAttentionì„ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°"""
    def __init__(self, d_model, num_heads, num_classes):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # Self-attention
        attn_output, attn_weights = self.attention(x, x, x)
        
        # Residual connection + layer norm
        x = self.norm(x + attn_output)
        
        # Global average pooling
        x = x.mean(dim=1)
        
        # Classification
        x = self.dropout(x)
        output = self.classifier(x)
        
        return output, attn_weights

def train_and_evaluate(model, data, num_epochs):
    """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""
    X, y = data
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    start_time = time.time()
    
    model.train()
    for epoch in range(num_epochs):
        # ë¯¸ë‹ˆë°°ì¹˜ ì²˜ë¦¬ (ê°„ë‹¨íˆ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
        optimizer.zero_grad()
        
        outputs, _ = model(X)
        loss = criterion(outputs, y)
        
        loss.backward()
        optimizer.step()
    
    training_time = time.time() - start_time
    
    # í‰ê°€
    model.eval()
    with torch.no_grad():
        outputs, _ = model(X)
        predicted = torch.argmax(outputs, dim=1)
        accuracy = accuracy_score(y.numpy(), predicted.numpy())
    
    return accuracy, training_time

def count_parameters(model):
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def plot_head_comparison(results):
    """Head ìˆ˜ ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
    head_nums = [r['num_heads'] for r in results]
    accuracies = [r['accuracy'] for r in results]
    times = [r['training_time'] for r in results]
    params = [r['params'] for r in results]
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # ì •í™•ë„
    axes[0].plot(head_nums, accuracies, 'o-', color='blue')
    axes[0].set_xlabel('Number of Heads')
    axes[0].set_ylabel('Accuracy')
    axes[0].set_title('ì •í™•ë„ vs Head ìˆ˜')
    axes[0].grid(True)
    
    # í›ˆë ¨ ì‹œê°„
    axes[1].plot(head_nums, times, 'o-', color='red')
    axes[1].set_xlabel('Number of Heads')
    axes[1].set_ylabel('Training Time (s)')
    axes[1].set_title('í›ˆë ¨ ì‹œê°„ vs Head ìˆ˜')
    axes[1].grid(True)
    
    # íŒŒë¼ë¯¸í„° ìˆ˜
    axes[2].plot(head_nums, params, 'o-', color='green')
    axes[2].set_xlabel('Number of Heads')
    axes[2].set_ylabel('Parameters')
    axes[2].set_title('íŒŒë¼ë¯¸í„° ìˆ˜ vs Head ìˆ˜')
    axes[2].grid(True)
    
    plt.tight_layout()
    plt.show()

# 4. ì‹¤ì œ ë¬¸ì¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
def test_with_real_sentence():
    """ì‹¤ì œ ë¬¸ì¥ìœ¼ë¡œ MultiHeadAttention í…ŒìŠ¤íŠ¸"""
    print("=== 4. ì‹¤ì œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸ ===")
    
    # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €
    vocab = {'<PAD>': 0, 'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6}
    sentence = "the cat sat on the mat"
    tokens = [vocab.get(word, 0) for word in sentence.split()]
    token_names = sentence.split()
    
    print(f"ë¬¸ì¥: {sentence}")
    print(f"í† í°ë“¤: {token_names}")
    
    # Embedding
    d_model = 128
    num_heads = 4
    embedding = nn.Embedding(len(vocab), d_model)
    
    # í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜
    token_tensor = torch.tensor(tokens).unsqueeze(0)
    x = embedding(token_tensor)
    
    # MultiHeadAttention ì ìš©
    mha = MultiHeadAttention(d_model, num_heads)
    output, attn_weights = mha(x, x, x)
    
    # ì‹œê°í™”
    visualize_head_patterns(mha, x, attn_weights, token_names)
    
    return output, attn_weights

# 5. Attention Head ë‹¤ì–‘ì„± ë¶„ì„
def analyze_head_diversity(attn_weights):
    """Head ê°„ì˜ ë‹¤ì–‘ì„± ë¶„ì„"""
    print("=== 5. Head ë‹¤ì–‘ì„± ë¶„ì„ ===")
    
    num_heads = attn_weights.size(1)
    
    # Head ê°„ correlation ê³„ì‚°
    correlations = []
    for i in range(num_heads):
        for j in range(i+1, num_heads):
            attn_i = attn_weights[0, i].flatten()
            attn_j = attn_weights[0, j].flatten()
            
            corr = torch.corrcoef(torch.stack([attn_i, attn_j]))[0, 1].item()
            correlations.append(corr)
    
    avg_correlation = np.mean(correlations)
    print(f"Head ê°„ í‰ê·  ìƒê´€ê´€ê³„: {avg_correlation:.3f}")
    
    if avg_correlation < 0.5:
        print("âœ… Headë“¤ì´ ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸  Headë“¤ì´ ìœ ì‚¬í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    plt.figure(figsize=(8, 6))
    corr_matrix = np.zeros((num_heads, num_heads))
    
    idx = 0
    for i in range(num_heads):
        for j in range(i+1, num_heads):
            corr_matrix[i, j] = correlations[idx]
            corr_matrix[j, i] = correlations[idx]
            idx += 1
    
    # ëŒ€ê°ì„ ì€ 1ë¡œ ì„¤ì •
    np.fill_diagonal(corr_matrix, 1.0)
    
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, square=True)
    plt.title('Head ê°„ Attention Pattern ìƒê´€ê´€ê³„')
    plt.xlabel('Head')
    plt.ylabel('Head')
    plt.show()

# ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def run_multihead_validation():
    """ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
    print("ğŸ¯ MultiHeadAttention ê²€ì¦ ì‹œì‘!")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ ê²€ì¦
    mha, x, attn_weights = test_dimensions()
    
    # 2. ì‹¤ì œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸
    output, attn_weights = test_with_real_sentence()
    
    # 3. Head ë‹¤ì–‘ì„± ë¶„ì„
    analyze_head_diversity(attn_weights)
    
    # 4. Head ìˆ˜ ë¹„êµ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
    print("\nâ° Head ìˆ˜ ë¹„êµëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
    choice = input().lower()
    if choice == 'y':
        results = compare_head_numbers()
        return results
    
    print("\nğŸ‰ ê²€ì¦ ì™„ë£Œ!")

if __name__ == "__main__":
    run_multihead_validation()